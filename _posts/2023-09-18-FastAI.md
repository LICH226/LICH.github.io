layout: post
title: FastAI基础知识
date: 2023-09-18
Author: YMieMie
tags: [ NLP]
toc: true
comments: true



本博客记录Fastai的简单入门学习过程，掌握基本的基础架构知识。



# About fastai

fastai 是一个深度学习库，它为从业者提供高级组件，可以快速轻松地提供标准深度学习领域最先进的结果，并为研究人员提供可以混合和匹配构建的低级组件 新方法。 它的目标是在不牺牲易用性、灵活性或性能的情况下实现这两件事。 这要归功于精心分层的架构，它以解耦抽象的形式表达了许多深度学习和数据处理技术的共同底层模式。 利用底层 Python 语言的活力和 PyTorch 库的灵活性，可以简洁、清晰地表达这些抽象。





1. 一个优化器，将现代优化器的常见功能重构为两个基本部分，允许用 4-5 行代码实现优化算法
2. 一种新颖的双向回调系统，可以访问数据、模型或优化器的任何部分，并在训练期间随时更改它
3. 新的数据块API

[![pPoz20U.md.png](https://z1.ax1x.com/2023/09/23/pPoz20U.md.png)](https://imgse.com/i/pPoz20U)

## 从其他库迁移

从普通的 PyTorch、Ignite 或任何其他基于 PyTorch 的库迁移非常容易，甚至可以将 fastai 与其他库结合使用。 一般来说，您将能够使用所有现有的数据处理代码，但能够减少训练所需的代码量，并更轻松地利用现代最佳实践。

# Transformers

在本教程中，我们将了解如何使用 fastai 库通过 HuggingFace 从 Transformers 库中微调预训练的 Transformer 模型。 我们将使用中级 API 来收集数据。 

## Importing a transformers pretrained model

```python
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
```

您可以通过更改 pretrained_weights 的内容来更改使用的模型（如果不是 GPT2 模型，您当然需要更改用于model类和tokenizer 类）。

```python
pretrained_weights = 'gpt2'
tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)
model = GPT2LMHeadModel.from_pretrained(pretrained_weights)
```

HuggingFace 中的分词器通常一步完成分词和数值化（我们暂时忽略填充警告）：

```python
ids = tokenizer.encode('This is an example of text, and')
ids
```

```
[1212, 318, 281, 1672, 286, 2420, 11, 290]
```

```python
tokenizer.decode(ids)
```

```
'This is an example of text, and'
```

该model可用于生成预测（已预先训练）。 它有一个生成方法，需要一些prompt，因此我们向它提供我们的 ids 并添加一个批次维度（我们也可以忽略一个填充警告）：

```python
import torch
t = torch.LongTensor(ids)[None]
preds = model.generate(t)
```

```
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
```

```python
tokenizer.decode(preds[0].numpy())
```

```
"This is an example of text, and it's not a good one.\n\nThe first thing"
```

## Bridging the gap with fastai



现在让我们看看如何使用 fastai 来微调 wikitext-2 上的模型，使用所有训练实用程序（learning rate finder, 1cycle policy等）。 首先，我们导入所有文本实用程序：

```python
from fastai.text.all import *
```

## Preparing the data

```python
path = untar_data(URLs.WIKITEXT_TINY)
path.ls()
```

```python
df_train = pd.read_csv(path/'train.csv', header=None)
df_valid = pd.read_csv(path/'test.csv', header=None)
df_train.head()
```

我们将所有文本收集在一个 numpy 数组中（因为在 fastai 中使用这种方式会更容易）：

```python
all_texts = np.concatenate([df_train[0].values, df_valid[0].values])
```

为了处理这些数据来训练模型，我们需要构建一个 Transform。 在这种情况下，我们可以一次性地进行预处理，并且只使用转换进行解码（我们将在之后看到如何进行），但是 HuggingFace 的快速分词器，正如其名称所示，速度很快，因此它不会这样做确实会影响性能。



In a fastai `Transform` you can define:

1. an `encodes` method that is applied when you call the transform (a bit like the `forward` method in a `nn.Module`)
2. a `decodes` method that is applied when you call the `decode` method of the transform, if you need to decode anything for showing purposes (like converting ids to a text here)
3. a `setups` method that sets some inner state of the `Transform` (not needed here so we skip it)

```python
class TransformersTokenizer(Transform):
    def __init__(self, tokenizer): self.tokenizer = tokenizer
    def encodes(self, x): 
        toks = self.tokenizer.tokenize(x)
        return tensor(self.tokenizer.convert_tokens_to_ids(toks))
    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))
```

1. 在编码中，我们不使用 tokenizer.encode 方法，因为它在标记化和数值化之后对模型进行了一些额外的预处理（之前抛出警告的部分）。 这里我们不需要任何后期处理，所以可以跳过它。
2. 在解码中，我们返回一个 TitledStr 对象，而不仅仅是一个普通字符串。 这是一个 fastai 类，它向字符串添加了一个 show 方法，这将允许我们使用所有 fastai show 方法。



然后，您可以使用 TfmdLists 通过此转换对数据进行分组。 它的名称中有一个 s，因为它包含训练集和验证集。 我们用分割表示训练集和验证集的索引（这里是直到 len(df_train) 之前的所有第一个索引，然后是所有剩余的索引）：

```python
splits = [range_of(df_train), list(range(len(df_train), len(all_texts)))]
tls = TfmdLists(all_texts, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)
```

当我们将此 TfmdLists 转换为 DataLoaders 时，我们指定 dl_type=LMDataLoader：我们将使用 LMDataLoader，因为我们是语言模型问题，而不是通常的 fastai TfmdDL。

```python
tls.train[0],tls.valid[0]
```

```
(tensor([220, 198, 796,  ..., 198, 220, 198]),
 tensor([220, 198, 796,  ..., 198, 220, 198]))
```

```python
tls.tfms(tls.train.items[0]).shape, tls.tfms(tls.valid.items[0]).shape
```

```
(torch.Size([4576]), torch.Size([1485]))
```

```python
show_at(tls.train, 0)
show_at(tls.valid, 0)
```

fastai 库期望将数据组装在 DataLoaders 对象（具有训练和验证数据加载器的对象）中。 我们可以使用 dataloaders 方法获得一个。 我们只需指定批量大小和序列长度。 我们将使用大小为 256 的序列进行训练（GPT2 使用序列长度 1024，但并非每个人都有足够的 GPU RAM 来进行训练）：

```python
bs,sl = 4,256
dls = tls.dataloaders(bs=bs, seq_len=sl)
```

## Fine-tuning the model

HuggingFace 模型将在输出中返回一个元组，其中包含实际预测和一些附加激活（我们是否希望在某些正则化方案中使用它们）。 为了在 fastai 训练循环中工作，我们需要使用回调来删除那些：我们使用它们来改变训练循环的行为。



这里我们需要编写事件 after_pred 并仅用其第一个元素替换 self.learn.pred （其中包含将传递给损失函数的预测）。 在回调中，有一个快捷方式可以让您访问任何底层 Learner 属性，因此我们可以编写 self.pred[0] 而不是 self.learn.pred[0]。 该快捷方式仅适用于读取访问，不适用于写入，因此我们必须在右侧编写 self.learn.pred （否则我们将在回调中设置 pred 属性）。

```python
class DropOutput(Callback):
    def after_pred(self): self.learn.pred = self.pred[0]
```

现在，我们准备创建 Learner，它是一个 fastai 对象，对数据、模型和损失函数进行分组，并处理模型训练或推理。 由于我们处于语言模型设置中，因此我们将困惑度作为度量传递，并且需要使用刚刚定义的回调。 最后，我们使用混合精度来节省尽可能多的内存（如果您有现代 GPU，它也会使训练速度更快）：

```python
learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity()).to_fp16()
```

```python
learn.validate()
```

```
(#2) [3.2537169456481934,25.88637924194336]
```

现在我们有了一个 Learner，我们可以使用所有 fastai 训练循环功能：学习率查找器、1cycle 训练等……

```python
learn.lr_find()
```

```
SuggestedLRs(lr_min=0.017378008365631102, lr_steep=0.14454397559165955)
```

[![pPozgmT.md.png](https://z1.ax1x.com/2023/09/23/pPozgmT.md.png)

```python
learn.fit_one_cycle(1, 1e-4)
```

| epoch | train_loss | valid_loss | perplexity | time  |
| ----- | ---------- | ---------- | ---------- | ----- |
| 0     | 2.986238   | 2.721945   | 15.209874  | 04:56 |

```python
prompt = "\n = Unicorn = \n \n A unicorn is a magical creature with a rainbow tail and a horn"
```

提示需要标记化和数字化，因此在使用模型的生成方法之前，我们使用与之前相同的函数来执行此操作。

```python
prompt_ids = tokenizer.encode(prompt)
inp = tensor(prompt_ids)[None].cuda()
inp.shape
```

```
torch.Size([1, 21])
```

```python
preds = learn.model.generate(inp, max_length=40, num_beams=5, temperature=1.5)
```

```
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
```

```python
tokenizer.decode(preds[0].cpu().numpy())
```

```
'\n = Unicorn = \n \n A unicorn is a magical creature with a rainbow tail and a horn @-@ shaped head. It is a member of the <unk> family of <unk'
```

