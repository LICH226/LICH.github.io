---
layout: post
title: 论文精度-ULMFiT
date: 2023-09-25
Author: YMieMie
tags: [ NLP]
toc: true
comments: true
---

对论文《Universal Language Model Fine-tuning for Text Classification》的阅读笔记。



# 文章摘要

在本文发表之前，迁移学习在计算机视觉方面取得了很多成功，而在自然语言处理还没有太多进展，仅仅停留在使用 word2vec 或多任务学习的阶段。本文提出了一个基于微调的通用语言模型，从而实现了 NLP 任务的类 CV 迁移学习。



论文地址：[Universal Language Model Fine-tuning for Text Classification - ACL Anthology](https://aclanthology.org/P18-1031/)

## 如何做基于参数的迁移学习

本文主要考虑的是基于参数的迁移学习。对于一个深度学习模型（CV 模型或 NLP 模型），其满足“越底层的特征越通用，越顶层的特征越特殊”的特点，因此我们可以将其分为两部分，底部是一个通用的模型，由一个大型数据集预训练得到，其能够使小数据集避免从头开始训练（随机初始化参数通常比预训练参数微淘要差），顶部则根据下游任务从头开始训练。

NLP 模型相对于 CV 模型较浅，因此文中也针对这一问题提出了适合 NLP 的微调技巧。

## 什么是 Universal Language Model

ULMFiT 方法分为以下几个阶段

- **LM pre-training**：此阶段借鉴 CV 中的 ImageNet，在通用领域语料库（文中用的是Wikitext-103，其包含 28,595 篇维基百科文章和 1030 亿个单词）上训练语言模型（文中用的是 merity 等人提出的 [AWD-LSTM](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1708.02182.pdf)），从而捕获不同层次文本的一般特征。
- **LM fine-tuning**：此阶段的目标是针对 target domain 上微调 Language Model。在这个阶段，作者用了 Discriminative fine-tuning 和 Slanted triangular learning rates 两个 tricks，以学习不同层次文本在 target domain 上的特征。
- **Classifier fine-tuning**：此阶段主要是训练模型的顶层结构。在这个阶段，作者用了 Concat pooling，Gradual unfreezing，BPTT for Text Classification (BPT3C) 和 Bidirectional language model 四个 tricks。

[![pP7RsUO.png](https://z1.ax1x.com/2023/09/25/pP7RsUO.png)](https://imgse.com/i/pP7RsUO)

## LM fine-tuning 的两个技巧

在 LM fine-tuning 阶段，作者提出了 Discriminative fine-tuning（Discr） 和 Slanted triangular learning rates（STLR） 两个技巧：

- Discr：由于底层特征更具有通用性，而顶层特征更具有特殊性，所以作者在训练过程中，对于不同层设置了不同的学习率（底层设置相对较小的学习率，顶层设置相对较大的学习率）。这里作者在具体实验中的操作是：先通过只调整最后一层的参数确定一个合适的学习率，然后往前的每一层都使用后一层学习率的 1/2.6 倍。
- STLR：学习率先增后减。先用较小的学习率，得到一个好的优化方向，再用较大的学习率进行优化，在训练后期再使用较小的学习率进行更细致的优化。这种学习率的变化曲线很像一个三角形，这也是为什么用“triangular”来形容学习率的原因。

## Classifier fine-tuning 的四个技巧

在 Classifier fine-tuning 阶段，作者提出了 Concat pooling，Gradual unfreezing，BPTT for Text Classification (BPT3C) 和 Bidirectional language model 四个技巧：

- Concat pooling：如果仅使用 RNN 模型最后一个 time step 的输出，显然会丢失信息，尤其是在长文本建模中，因此作者对 RNN 所有 time state 的 hidden states 进行 max pooling 和 mean pooling，然后将 pooling 得到的两个特征与最后一个 time step 的输出连接，作为最终输出。
- Gradual unfreezing：直接 fine-tuning 整个网络可能导致网络遗忘之前预训练得到的通用特征，因此作者提出了gradual unfreezing，具体做法是自顶向下以 epoch 为单位逐步进行 fine-tuning，即第一个 epoch 只解冻最后一层，第二个 epoch 解冻最后两层，以此类推。
- BPTT for Text Classification (BPT3C)：为了使大型文档的分类器微调可行，作者将文档划分为大小为 b 的固定长度批次。 在每个批次的开头，用前一批次的最终状态初始化模型，跟踪平均值和最大池的隐藏状态，梯度反向传播到批次。
- Bidirectional language model：本文分别训练了前向和后向的 Language Model，在 fine-tuning 阶段对预测的结果取平均。

## 总结

此文提出的 ULMFiT 模型与近两年的 Elmo，Bert 等，引领了 NLP 领域的预训练潮流，值得后来者反复研读，尤其是文章中提到的一些 tricks，也富有实用性。
